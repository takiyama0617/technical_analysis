{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "有報キャッチャーからXBRLのURLを取得する.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/takiyama0617/technical_analysis/blob/master/%E6%9C%89%E5%A0%B1%E3%82%AD%E3%83%A3%E3%83%83%E3%83%81%E3%83%A3%E3%83%BC%E3%81%8B%E3%82%89XBRL%E3%81%AEURL%E3%82%92%E5%8F%96%E5%BE%97%E3%81%99%E3%82%8B.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iR-MVi8gc5i0",
        "colab_type": "text"
      },
      "source": [
        "# １．有報キャッチャーからURLを取得\n",
        "XBRLのURLリスト dat_download{ str_period }.csv が吐き出される"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZyaRiITvejI",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import csv\n",
        "import time\n",
        "import re\n",
        "import requests\n",
        "import xml.etree.ElementTree as ET\n",
        "from tqdm import tqdm\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "\n",
        "class yuho_catcher():\n",
        "    def __init__(self, since, until, base_dir=None):\n",
        "        self.csv_tag = ['id', 'title', 'cd', 'url', 'update']\n",
        "        self.encode_type = 'utf-8'\n",
        "        self.wait_time = 2  # 間隔が短いと制限がかかる\n",
        "        self.base_url = 'http://resource.ufocatch.com/atom/edinetx/'\n",
        "        self.namespace = '{http://www.w3.org/2005/Atom}'\n",
        "        self.out_of_since = False\n",
        "        self.since = since\n",
        "        self.until = until\n",
        "        self.file_info_str = since.strftime(\n",
        "            '_%y%m%d_') + until.strftime('%y%m%d')\n",
        "        self.base_path = f'{ os.getcwd() if base_dir==None else base_dir }'\n",
        "\n",
        "    def get_link_info_str(self, ticker_symbol):\n",
        "        url = self.base_url + str(ticker_symbol)\n",
        "        count, retry = 0, 3\n",
        "        while True:\n",
        "            try:\n",
        "                response = requests.get(url)\n",
        "                return response.text\n",
        "            except Exception:\n",
        "                print(f'{ticker_symbol} のアクセスに失敗しました。[ {count} ]')\n",
        "                if count < retry:\n",
        "                    count += 1\n",
        "                    time.sleep(3)\n",
        "                    continue\n",
        "                else:\n",
        "                    raise\n",
        "\n",
        "    def parse_xml(self, string):\n",
        "        ET_tree = ET.fromstring(string)\n",
        "        ET.register_namespace('', self.namespace[1:-1])\n",
        "        return ET_tree\n",
        "\n",
        "    def get_link(self, tree):\n",
        "        yuho_dict = {}\n",
        "        # xmlのentry毎にfor\n",
        "        for el in tree.findall('.//'+self.namespace+'entry'):\n",
        "            title = el.find(self.namespace+'title').text\n",
        "            if not self.is_yuho(title):\n",
        "                continue\n",
        "            updated = el.find(self.namespace+'updated').text\n",
        "            checked = self.time_check(updated)\n",
        "            if not checked['until']:\n",
        "                continue\n",
        "            if not checked['since']:\n",
        "                self.out_of_since = True\n",
        "                return yuho_dict\n",
        "            _id = el.find(self.namespace+'id').text\n",
        "            links = el.findall('./'+self.namespace+'link[@type=\"text/xml\"]')\n",
        "            for link in links:\n",
        "                if '.xbrl' in link.attrib['href'] and 'PublicDoc' in link.attrib['href']:\n",
        "                    url = link.attrib['href']\n",
        "                    break\n",
        "                else:\n",
        "                    continue\n",
        "            else:\n",
        "                continue\n",
        "            cd = re.sub(r'^【(\\w+)】.*', r\"\\1\", title)\n",
        "            yuho_dict[_id] = {'id': _id, 'title': title,\n",
        "                              'cd': cd, 'url': url, 'update': updated}\n",
        "        return yuho_dict\n",
        "\n",
        "    def is_yuho(self, title):\n",
        "        if u'有価証券報告書' in str(title) and u'株式会社' in str(title) and u'内国投資信託受益証券' not in str(title):\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "    def time_check(self, update):\n",
        "        updated_time = datetime.strptime(update, '%Y-%m-%dT%H:%M:%S+09:00')\n",
        "        return {'since': updated_time >= self.since, 'until': updated_time < self.until}\n",
        "\n",
        "    def dump_file(self, file, info_dict, tag, encode_type):\n",
        "        with open(os.path.join(self.base_path, file), 'w', encoding=encode_type) as of:\n",
        "            writer = csv.DictWriter(of, tag, lineterminator='\\n')\n",
        "            writer.writeheader()\n",
        "            for key in info_dict:\n",
        "                writer.writerow(info_dict[key])\n",
        "\n",
        "    def craete_xbrl_url_json_each_symbols(self, list_symbols):\n",
        "        print(\n",
        "            f'since:{ self.since } ,until:{ self.until } ({ self.file_info_str })')\n",
        "        i, result_dict = 0, {}\n",
        "        for t_symbol in tqdm(list_symbols):\n",
        "            response_string = self.get_link_info_str(f'query/{t_symbol}')\n",
        "            ET_tree = self.parse_xml(response_string)\n",
        "            info_dict = self.get_link(ET_tree)\n",
        "            if len(info_dict) > 0:\n",
        "                for key in info_dict:\n",
        "                    result_dict[key] = info_dict[key]\n",
        "                self.dump_file(\n",
        "                    f'dat_download{ self.file_info_str }.csv', result_dict, self.csv_tag, self.encode_type)\n",
        "            time.sleep(self.wait_time)\n",
        "            i += 1\n",
        "        print('complete a download!!')\n",
        "\n",
        "    def craete_xbrl_url_json_from_latest(self):\n",
        "        print(\n",
        "            f'since:{ self.since } ,until:{ self.until } ({ self.file_info_str })')\n",
        "        i, result_dict = 0, {}\n",
        "        while True:\n",
        "            page = 1 + i\n",
        "            print(f'page{page}, loading...')\n",
        "            response_string = self.get_link_info_str(page)\n",
        "            ET_tree = self.parse_xml(response_string)\n",
        "            info_dict = self.get_link(ET_tree)\n",
        "            if len(info_dict) > 0:\n",
        "                for key in info_dict:\n",
        "                    result_dict[key] = info_dict[key]\n",
        "                self.dump_file(\n",
        "                    f'dat_download{ self.file_info_str }.csv', result_dict, self.csv_tag, self.encode_type)\n",
        "            time.sleep(self.wait_time)\n",
        "            if self.out_of_since:\n",
        "                break\n",
        "            i += 1\n",
        "        print('complete a download!!')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R9fbANxfdKH7",
        "colab_type": "code",
        "outputId": "1c87a038-4e37-4322-e624-c12567077446",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "from datetime import datetime\n",
        "\n",
        "since = datetime.strptime('2020-04-01' ,'%Y-%m-%d')\n",
        "until = datetime.strptime('2020-04-26' ,'%Y-%m-%d')\n",
        "yuho = yuho_catcher( since ,until )\n",
        "yuho.craete_xbrl_url_json_from_latest()\n",
        "# yuho.craete_xbrl_url_json_each_symbols( [7203] )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "since:2020-04-01 00:00:00 ,until:2020-04-26 00:00:00 (_200401_200426)\n",
            "page1, loading...\n",
            "page2, loading...\n",
            "page3, loading...\n",
            "page4, loading...\n",
            "page5, loading...\n",
            "page6, loading...\n",
            "page7, loading...\n",
            "page8, loading...\n",
            "page9, loading...\n",
            "page10, loading...\n",
            "complete a download!!\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}