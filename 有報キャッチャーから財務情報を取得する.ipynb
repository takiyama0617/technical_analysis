{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "有報キャッチャーから財務情報を取得する.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/takiyama0617/technical_analysis/blob/master/%E6%9C%89%E5%A0%B1%E3%82%AD%E3%83%A3%E3%83%83%E3%83%81%E3%83%A3%E3%83%BC%E3%81%8B%E3%82%89%E8%B2%A1%E5%8B%99%E6%83%85%E5%A0%B1%E3%82%92%E5%8F%96%E5%BE%97%E3%81%99%E3%82%8B.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TOVUZGRzUsje",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "12c07e3e-18ba-46c8-f656-039faa09957f"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "BASE_DIR = f'/content/drive/My Drive/workspace/technical_analysis/data/'\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iR-MVi8gc5i0",
        "colab_type": "text"
      },
      "source": [
        "# 1.有報キャッチャーからURLを取得\n",
        "XBRLのURLリスト dat_download{ str_period }.csv が吐き出される"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZyaRiITvejI",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import csv\n",
        "import time\n",
        "import re\n",
        "import requests\n",
        "import xml.etree.ElementTree as ET\n",
        "from tqdm import tqdm\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "\n",
        "class yuho_catcher():\n",
        "    def __init__(self, since, until, base_dir=None):\n",
        "        self.csv_tag = ['id', 'title', 'cd', 'url', 'update']\n",
        "        self.encode_type = 'utf-8'\n",
        "        self.wait_time = 2  # 間隔が短いと制限がかかる\n",
        "        self.base_url = 'http://resource.ufocatch.com/atom/edinetx/'\n",
        "        self.namespace = '{http://www.w3.org/2005/Atom}'\n",
        "        self.out_of_since = False\n",
        "        self.since = since\n",
        "        self.until = until\n",
        "        self.file_info_str = since.strftime(\n",
        "            '_%y%m%d_') + until.strftime('%y%m%d')\n",
        "        self.base_path = f'{ os.getcwd() if base_dir==None else base_dir }'\n",
        "\n",
        "    def get_link_info_str(self, ticker_symbol):\n",
        "        url = self.base_url + str(ticker_symbol)\n",
        "        count, retry = 0, 3\n",
        "        while True:\n",
        "            try:\n",
        "                response = requests.get(url)\n",
        "                return response.text\n",
        "            except Exception:\n",
        "                print(f'{ticker_symbol} のアクセスに失敗しました。[ {count} ]')\n",
        "                if count < retry:\n",
        "                    count += 1\n",
        "                    time.sleep(3)\n",
        "                    continue\n",
        "                else:\n",
        "                    raise\n",
        "\n",
        "    def parse_xml(self, string):\n",
        "        ET_tree = ET.fromstring(string)\n",
        "        ET.register_namespace('', self.namespace[1:-1])\n",
        "        return ET_tree\n",
        "\n",
        "    def get_link(self, tree):\n",
        "        yuho_dict = {}\n",
        "        # xmlのentry毎にfor\n",
        "        for el in tree.findall('.//'+self.namespace+'entry'):\n",
        "            title = el.find(self.namespace+'title').text\n",
        "            if not self.is_yuho(title):\n",
        "                continue\n",
        "            updated = el.find(self.namespace+'updated').text\n",
        "            checked = self.time_check(updated)\n",
        "            if not checked['until']:\n",
        "                continue\n",
        "            if not checked['since']:\n",
        "                self.out_of_since = True\n",
        "                return yuho_dict\n",
        "            _id = el.find(self.namespace+'id').text\n",
        "            links = el.findall('./'+self.namespace+'link[@type=\"text/xml\"]')\n",
        "            for link in links:\n",
        "                if '.xbrl' in link.attrib['href'] and 'PublicDoc' in link.attrib['href']:\n",
        "                    url = link.attrib['href']\n",
        "                    break\n",
        "                else:\n",
        "                    continue\n",
        "            else:\n",
        "                continue\n",
        "            cd = re.sub(r'^【(\\w+)】.*', r\"\\1\", title)\n",
        "            yuho_dict[_id] = {'id': _id, 'title': title,\n",
        "                              'cd': cd, 'url': url, 'update': updated}\n",
        "        return yuho_dict\n",
        "\n",
        "    def is_yuho(self, title):\n",
        "        if u'有価証券報告書' in str(title) and u'株式会社' in str(title) and u'内国投資信託受益証券' not in str(title):\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "    def time_check(self, update):\n",
        "        updated_time = datetime.strptime(update, '%Y-%m-%dT%H:%M:%S+09:00')\n",
        "        return {'since': updated_time >= self.since, 'until': updated_time < self.until}\n",
        "\n",
        "    def dump_file(self, file, info_dict, tag, encode_type):\n",
        "        with open(os.path.join(self.base_path, file), 'w', encoding=encode_type) as of:\n",
        "            writer = csv.DictWriter(of, tag, lineterminator='\\n')\n",
        "            writer.writeheader()\n",
        "            for key in info_dict:\n",
        "                writer.writerow(info_dict[key])\n",
        "\n",
        "    def craete_xbrl_url_json_each_symbols(self, list_symbols):\n",
        "        print(\n",
        "            f'since:{ self.since } ,until:{ self.until } ({ self.file_info_str })')\n",
        "        i, result_dict = 0, {}\n",
        "        for t_symbol in tqdm(list_symbols):\n",
        "            response_string = self.get_link_info_str(f'query/{t_symbol}')\n",
        "            ET_tree = self.parse_xml(response_string)\n",
        "            info_dict = self.get_link(ET_tree)\n",
        "            if len(info_dict) > 0:\n",
        "                for key in info_dict:\n",
        "                    result_dict[key] = info_dict[key]\n",
        "                self.dump_file(\n",
        "                    f'dat_download{ self.file_info_str }.csv', result_dict, self.csv_tag, self.encode_type)\n",
        "            time.sleep(self.wait_time)\n",
        "            i += 1\n",
        "        print('complete a download!!')\n",
        "\n",
        "    def craete_xbrl_url_json_from_latest(self):\n",
        "        print(\n",
        "            f'since:{ self.since } ,until:{ self.until } ({ self.file_info_str })')\n",
        "        i, result_dict = 0, {}\n",
        "        while True:\n",
        "            page = 1 + i\n",
        "            print(f'page{page}, loading...')\n",
        "            response_string = self.get_link_info_str(page)\n",
        "            ET_tree = self.parse_xml(response_string)\n",
        "            info_dict = self.get_link(ET_tree)\n",
        "            if len(info_dict) > 0:\n",
        "                for key in info_dict:\n",
        "                    result_dict[key] = info_dict[key]\n",
        "                self.dump_file(\n",
        "                    f'dat_download{ self.file_info_str }.csv', result_dict, self.csv_tag, self.encode_type)\n",
        "            time.sleep(self.wait_time)\n",
        "            if self.out_of_since:\n",
        "                break\n",
        "            i += 1\n",
        "        print('complete a download!!')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R9fbANxfdKH7",
        "colab_type": "code",
        "outputId": "d5fdef4a-9f04-4fd5-d66d-d0ef9633eaea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "from datetime import datetime\n",
        "\n",
        "since = datetime.strptime('2020-04-01' ,'%Y-%m-%d')\n",
        "until = datetime.strptime('2020-05-03' ,'%Y-%m-%d')\n",
        "yuho = yuho_catcher( since ,until, base_dir=BASE_DIR)\n",
        "yuho.craete_xbrl_url_json_from_latest()\n",
        "# yuho.craete_xbrl_url_json_each_symbols( [7203] )"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "since:2020-04-01 00:00:00 ,until:2020-05-03 00:00:00 (_200401_200503)\n",
            "page1, loading...\n",
            "page2, loading...\n",
            "page3, loading...\n",
            "page4, loading...\n",
            "page5, loading...\n",
            "page6, loading...\n",
            "page7, loading...\n",
            "page8, loading...\n",
            "page9, loading...\n",
            "page10, loading...\n",
            "page11, loading...\n",
            "complete a download!!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mUhSG9RcO4VO",
        "colab_type": "text"
      },
      "source": [
        "# 2.XBRLのURLリストからファイルをダウンロードし、CSV変換\n",
        "\n",
        "dat_download_{from}_{to}.csvからXBRLファイルをダウンロード\n",
        "\n",
        "ダウンロードしたXBRLファイルから必要情報を抜き出し、eggs_{from}_{to}.csvを作成する\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fr5Ad1JLRNxJ",
        "colab_type": "code",
        "outputId": "18efe74c-fca6-4dbe-a3da-f9b5f1353324",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "!pip install python-xbrl"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: python-xbrl in /usr/local/lib/python3.6/dist-packages (1.1.1)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.6/dist-packages (from python-xbrl) (3.6.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from python-xbrl) (1.12.0)\n",
            "Requirement already satisfied: ordereddict in /usr/local/lib/python3.6/dist-packages (from python-xbrl) (1.1)\n",
            "Requirement already satisfied: marshmallow in /usr/local/lib/python3.6/dist-packages (from python-xbrl) (3.5.2)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.6/dist-packages (from python-xbrl) (4.2.6)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.6/dist-packages (from python-xbrl) (4.6.3)\n",
            "Requirement already satisfied: pep8 in /usr/local/lib/python3.6/dist-packages (from python-xbrl) (1.7.1)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pytest->python-xbrl) (8.2.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest->python-xbrl) (19.3.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from pytest->python-xbrl) (46.1.3)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.6/dist-packages (from pytest->python-xbrl) (1.3.0)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pytest->python-xbrl) (1.8.1)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.6/dist-packages (from pytest->python-xbrl) (0.7.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i3Kq_6sfOzFm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import re\n",
        "import csv\n",
        "import time\n",
        "import urllib\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from datetime import datetime, timedelta\n",
        "from dateutil.relativedelta import relativedelta\n",
        "from xbrl import XBRLParser\n",
        "\n",
        "default_tag = ['file_nm', 'element_id', 'amount']\n",
        "custom_tag = ['unit_ref', 'decimals', 'contextref']\n",
        "encode_type = 'utf-8'\n",
        "\n",
        "\n",
        "class XbrlParser(XBRLParser):\n",
        "    def __init__(self, info_dict):\n",
        "        self.info_dict = info_dict\n",
        "        self.xbrl_url = info_dict['url']\n",
        "        xbrl_filename = self.xbrl_url.split('/')[-1]\n",
        "        directory_path = f'{ os.getcwd() }/xbrl_files'\n",
        "        self.__make_directory(directory_path)\n",
        "        self.xbrl_filepath = f'{ directory_path }/{ xbrl_filename }.xbrl'\n",
        "\n",
        "    def __make_directory(self, dir_name):\n",
        "        if not os.path.exists(dir_name):\n",
        "            os.mkdir(dir_name)\n",
        "\n",
        "    def __download_xbrl_file(self):\n",
        "        counter, retry = 0, 3\n",
        "        while True:\n",
        "            try:\n",
        "                urllib.request.urlretrieve(self.xbrl_url, self.xbrl_filepath)\n",
        "                return True\n",
        "            except Exception as e:\n",
        "                if counter < retry:\n",
        "                    print(f'XBRLファイルのDLに失敗しました（{ counter }）{ self.xbrl_url }')\n",
        "                    time.sleep(3)\n",
        "                    counter += 1\n",
        "                else:\n",
        "                    print('上手く行かないので次の処理に移ります')\n",
        "                    return False\n",
        "\n",
        "    def parse_xbrl(self):\n",
        "        DL_is_succeeded = self.__download_xbrl_file()\n",
        "        if DL_is_succeeded:\n",
        "            with open(self.xbrl_filepath, 'r', encoding='utf-8') as of:\n",
        "                xbrl = XBRLParser.parse(of)  # beautiful soup type object\n",
        "        else:\n",
        "            return\n",
        "        result_dicts = {}\n",
        "        _idx = 0\n",
        "        name_space = 'jp*'\n",
        "        for node in xbrl.find_all(name=re.compile(name_space+':*')):\n",
        "            if self.ignore_pattern(node):\n",
        "                continue\n",
        "            row_dict = {}\n",
        "            row_dict['file_nm'] = self.xbrl_filepath.rsplit(os.sep, 1)[1]\n",
        "            row_dict['element_id'] = node.name\n",
        "            row_dict['amount'] = node.string\n",
        "            for tag in custom_tag:\n",
        "                row_dict[tag] = self.get_attrib_value(node, tag)\n",
        "            result_dicts[_idx] = row_dict\n",
        "            _idx += 1\n",
        "        return result_dicts\n",
        "\n",
        "    def ignore_pattern(self, node):\n",
        "        if 'xsi:nil' in node.attrs:\n",
        "            if node.attrs['xsi:nil'] == 'true':\n",
        "                return True\n",
        "        if not isinstance(node.string, str):\n",
        "            return True  # 結果が空の場合は対象外にする\n",
        "        if str(node.string).find(u'\\n') > -1:\n",
        "            return True  # 改行コードが結果にある場合対象外にする\n",
        "        if u'textblock' in str(node.name):\n",
        "            return True  # text文字列は解析に利用できないため、対象外\n",
        "        return False\n",
        "\n",
        "    def get_attrib_value(self, node, attrib):\n",
        "        if attrib in node.attrs.keys():\n",
        "            return node.attrs[attrib]\n",
        "        else:\n",
        "            return None\n",
        "\n",
        "\n",
        "class parse_operator():\n",
        "    def __init__(self, df, str_period, base_dir):\n",
        "        self.df = df\n",
        "        self.str_period = str_period\n",
        "        self.base_path = base_dir\n",
        "\n",
        "    def __dump_file(self, writer, info_dicts):\n",
        "        _idx = 0\n",
        "        while _idx < len(info_dicts):\n",
        "            row_dict = info_dicts[_idx]\n",
        "            writer.writerow(row_dict)\n",
        "            _idx += 1\n",
        "\n",
        "    def xbrl_to_csv(self):\n",
        "        file = f'eggs_{ self.str_period }.csv'\n",
        "        with open(os.path.join(self.base_path, file), 'w', encoding=encode_type) as of:\n",
        "            resultCsvWriter = csv.DictWriter(\n",
        "                of, default_tag + custom_tag, lineterminator='\\n')\n",
        "            resultCsvWriter.writeheader()\n",
        "            for i in tqdm(range(len(self.df.index))):\n",
        "                row = self.df.iloc[i]\n",
        "                xp = XbrlParser(row.to_dict())\n",
        "                info_dicts = xp.parse_xbrl()\n",
        "                self.__dump_file(resultCsvWriter, info_dicts)\n",
        "        print('completed conversions!!')\n",
        "\n",
        "\n",
        "def xbrl_operator(start, end, loop_month=1, restart=0, base_dir=None):\n",
        "    big_period = start.strftime('%y%m%d_')+end.strftime('%y%m%d')\n",
        "    base_path = f'{ os.getcwd() if base_dir==None else base_dir }'\n",
        "    file = f'dat_download_{ big_period }.csv'\n",
        "    info_df = pd.read_csv(os.path.join(base_path, file),\n",
        "                          parse_dates=['update'])\n",
        "    since = start + relativedelta(months=(restart))\n",
        "    until = start + relativedelta(months=(loop_month + restart))\n",
        "    until = until if until < end else end\n",
        "\n",
        "    while True:\n",
        "        target_df = info_df.loc[(info_df['update'] >= since) & (\n",
        "            info_df['update'] < until)]\n",
        "        small_period = since.strftime('%y%m%d_')+until.strftime('%y%m%d')\n",
        "        print(f'since:{ since } ,until:{ until } ({ small_period })')\n",
        "        xbrl_parse_operator = parse_operator(\n",
        "            target_df, small_period, base_path)\n",
        "        xbrl_parse_operator.xbrl_to_csv()\n",
        "        since = until\n",
        "        if since >= end:\n",
        "            break\n",
        "        until = until + relativedelta(months=loop_month)\n",
        "        until = until if until < end else end\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3_4QXZ9kRVc6",
        "colab_type": "code",
        "outputId": "a3faaff6-4a68-46d0-c80e-c15cad7ba781",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "from datetime import datetime\n",
        "\n",
        "start = datetime.strptime('2020-04-01 +0900', '%Y-%m-%d %z')\n",
        "end = datetime.strptime('2020-05-03 +0900', '%Y-%m-%d %z')\n",
        "xbrl_operator(start, end, 3, base_dir=BASE_DIR)\n"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/124 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "since:2020-04-01 00:00:00+09:00 ,until:2020-05-03 00:00:00+09:00 (200401_200503)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 124/124 [09:27<00:00,  4.58s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "completed conversions!!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}